{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e2debfe",
   "metadata": {},
   "source": [
    "# Fabric CLI and Deployment Pipelines\n",
    "This notebook provides an example of how to set up a deployment pipeline using the Fabric CLI with Python.\n",
    "The workflow is based on the FMD Framework (https://github.com/edkreuk/FMD_FRAMEWORK), which offers more examples and detailed guidance on deploying your own data estate with the CLI.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f053f7",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "# Also make sure the notebook uses Python and not PySpark\n",
    "%pip install ms-fabric-cli --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0195b9",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "import msal \n",
    "import os\n",
    "import subprocess\n",
    "import json\n",
    "from time import sleep, time\n",
    "import zipfile\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc738e6b-ebba-447a-86e7-eb5cc5e20d00",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ac9831",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "# Do not change the below params, some logic/code is depeneded on them. Or change them and also adjust the code such that it works again :)\n",
    "resource_dir = \"./builtin/src\"\n",
    "dev_suffix = \" (D)\"\n",
    "prod_suffix = \" (P)\"\n",
    "empty_guid = \"00000000-0000-0000-0000-000000000000\"\n",
    "mapping_table = []\n",
    "tasks = []\n",
    "\n",
    "# These are service principal params, no need to use them if you authenticate with a user\n",
    "SERVICE_PRINCIPAL_LOGIN = False                                                                 # Set to True if you want to use your own user account instead of the SP\n",
    "CLIENT_SECRET = \"0000~000000000000000_andprobalysometextidk\",                                   # Fill in the secret value for your SP\n",
    "CLIENT_ID = \"00000000-0000-0000-0000-00000a0a0a00\"                                              # The service principal (app registration) which can be found in Entra\n",
    "TENANT_ID = \"00ab00a0-00a0-0000-aaaa-0a0a0a0a0a0a\"                                              # The tenant id (can also be found in Entra)\n",
    "\n",
    "# Change these as you see fit\n",
    "# !!!!!!!!!!! A capacity name is required if you want to actually deploy stuff. Use the free trial capacity, if you still can :) !!!!!!!!!!!!!!!!\n",
    "CAPACITY_NAME = \"Trial-20250602T071858Z-0a0a0a0a0a0a0a0a0a0a0a\"                                # This should be the actual capacity name which you use, otherwise the code won't work\n",
    "FRAMEWORK_NAME = 'DPF'                                                                         # max 6 characters for better visibility, no spaces or weird !@#$^%^ stuff\n",
    "DEFAULT_ITEM_DESCRIPTION = \"This item is generated by the Deployment Pipeline Fabric notebook.\"            # Fill in w/e just know everyone with acces will see this ;)\n",
    "DEFAULT_WORKSPACE_DESCRIPTION = \"The items are created by the Deployment Pipeline Fabric notebook.\"        # Fill in w/e just know everyone with acces will see this ;)\n",
    "ENVIRONMENT_NAME = \"development\"                                                                # Items will only be deployed to this environment\n",
    "\n",
    "\n",
    "workspace_roles = [ # Keep emtpy [] if you only want to assign the current user)\n",
    "                    {\n",
    "                        \"principal\": {\n",
    "                            \"id\": '00000000-0000-0000-0000-000000000000',\n",
    "                            \"displayName\": \"Keep it safe\", # Name of the group or user to assign the role to\n",
    "                            \"type\": \"Group\"\n",
    "                        },\n",
    "                        \"role\": \"admin\"  #(choose from 'admin', 'member', 'contributor', 'viewer')\n",
    "                    },\n",
    "                    {\n",
    "                        \"principal\": {\n",
    "                            \"id\": '00000000-0000-0000-0000-000000000000',\n",
    "                            \"displayName\": \"ThisIs MyName\", # Name of the group or user to assign the role to\n",
    "                            \"type\": \"User\"\n",
    "                        },\n",
    "                        \"role\": \"admin\"  #(choose from 'admin', 'member', 'contributor', 'viewer')\n",
    "                    },\n",
    "                ]\n",
    "\n",
    "template_deployment_pipeline = {\n",
    "    \"displayName\": \"{workspace_name} Deployment Pipeline\",\n",
    "    \"description\": \"This deployment pipeline was created by Deployment Pipeline Fabric script.\",\n",
    "    \"stages\": [\n",
    "        {\n",
    "            \"displayName\": \"development\", # If you change this, also change the environment name such that they match\n",
    "            \"description\": \"development stage\",\n",
    "            \"isPublic\": False\n",
    "        },\n",
    "        {\n",
    "            \"displayName\": \"production\",\n",
    "            \"description\": \"production stage\",\n",
    "            \"isPublic\": True\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "environments = [\n",
    "    {\n",
    "        \"environment_name\": ENVIRONMENT_NAME,\n",
    "        \"workspaces\": {\n",
    "            \"code\": {\n",
    "                \"name\": FRAMEWORK_NAME + \" Code\" + dev_suffix, \n",
    "                \"roles\": workspace_roles,\n",
    "                \"capacity_name\": CAPACITY_NAME,\n",
    "                \"workspace_id\": empty_guid,\n",
    "                \"deployment_pipeline\": True # Set both to True or False, otherwise the different stages won't get added to the deployment pipeline.\n",
    "            },\n",
    "            \"data\": {\n",
    "                \"name\": FRAMEWORK_NAME + \" Data\" + dev_suffix, \n",
    "                \"roles\": workspace_roles,\n",
    "                \"capacity_name\": \"none\",\n",
    "                \"workspace_id\": empty_guid,\n",
    "                \"deployment_pipeline\": False\n",
    "            },\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"environment_name\": \"production\",\n",
    "        \"workspaces\": {\n",
    "            \"code\": {\n",
    "                \"name\": FRAMEWORK_NAME + \" Code\" + prod_suffix,\n",
    "                \"roles\": workspace_roles,\n",
    "                \"capacity_name\": CAPACITY_NAME,\n",
    "                \"workspace_id\": empty_guid,\n",
    "                \"deployment_pipeline\": True\n",
    "            },\n",
    "            \"data\": {\n",
    "                \"name\": FRAMEWORK_NAME + \" Data\" + prod_suffix,\n",
    "                \"roles\": workspace_roles,\n",
    "                \"capacity_name\": \"none\",\n",
    "                \"workspace_id\": empty_guid,\n",
    "                \"deployment_pipeline\": False\n",
    "            },\n",
    "        },\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf94bb3a-600e-417e-9f32-2ce7ae39605d",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# CLI Login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671b173f",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "# If you want to use the service principal account to deploy all your resources run this. Make sure you filled in all the required parameters.\n",
    "if SERVICE_PRINCIPAL_LOGIN:\n",
    "    def get_fabric_service_principal_token(client_id, client_secret, tenant_id):\n",
    "        app = msal.ConfidentialClientApplication(\n",
    "            client_id=client_id,\n",
    "            client_credential=client_secret,\n",
    "            authority=f\"https://login.microsoftonline.com/{tenant_id}\"\n",
    "        )\n",
    "        fabric_scope = \"https://analysis.windows.net/powerbi/api/.default\"\n",
    "        result = app.acquire_token_for_client(scopes=[fabric_scope])\n",
    "        if \"access_token\" in result:\n",
    "            return result[\"access_token\"]\n",
    "        else:\n",
    "            print(f\"Error: {result.get('error')}\")\n",
    "            print(f\"Description: {result.get('error_description')}\")\n",
    "            return None\n",
    "\n",
    "\n",
    "    fabric_token = get_fabric_service_principal_token(CLIENT_ID, CLIENT_SECRET, TENANT_ID)\n",
    "\n",
    "    if fabric_token:\n",
    "        print(\"Successfully obtained Fabric token\")\n",
    "    else:\n",
    "        print(\"Failed to obtain token\")\n",
    "\n",
    "    # Set environment so subprocess can acces the token\n",
    "    os.environ['FAB_TOKEN'] = fabric_token\n",
    "    os.environ['FAB_TOKEN_ONELAKE'] = fabric_token\n",
    "    os.environ['FAB_TOKEN_AZURE'] = fabric_token\n",
    "\n",
    "# Otherwise just use your current user\n",
    "else: \n",
    "    # Set environment parameters for Fabric CLI\n",
    "    token = notebookutils.credentials.getToken('pbi')\n",
    "    os.environ['FAB_TOKEN'] = token\n",
    "    os.environ['FAB_TOKEN_ONELAKE'] = token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199a81f4",
   "metadata": {},
   "source": [
    "# File download\n",
    "#### We use the original FMD repository as a source to download files that can be imported into a workspace and later deployed through the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ddb32e",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "##### DO NOT CHANGE UNLESS SPECIFIED OTHERWISE ####\n",
    "repo_owner = \"edkreuk\"              # Owner of the repository\n",
    "repo_name = \"FMD_FRAMEWORK\"         # Name of the repository\n",
    "branch = \"main\"                     #\"main\" is default                    \n",
    "folder_prefix = \"\"\n",
    "###################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1913fdac",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "def download_folder_as_zip(repo_owner, repo_name, output_zip, branch=\"main\", folder_to_extract=\"src\",  remove_folder_prefix = \"\"):\n",
    "    # Construct the URL for the GitHub API to download the repository as a zip file\n",
    "    url = f\"https://api.github.com/repos/{repo_owner}/{repo_name}/zipball/{branch}\"\n",
    "    \n",
    "    # Make a request to the GitHub API\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    folder_to_extract = f\"/{folder_to_extract}\" if folder_to_extract[0] != \"/\" else folder_to_extract\n",
    "    \n",
    "    # Ensure the directory for the output zip file exists\n",
    "    os.makedirs(os.path.dirname(output_zip), exist_ok=True)\n",
    "    \n",
    "    # Create a zip file in memory\n",
    "    with zipfile.ZipFile(BytesIO(response.content)) as zipf:\n",
    "        with zipfile.ZipFile(output_zip, 'w') as output_zipf:\n",
    "            file_count = 0\n",
    "            for file_info in zipf.infolist():\n",
    "                if file_count > 20: # Added a file count because notebook storage cannot exceed a certain limit\n",
    "                    break\n",
    "\n",
    "                parts = file_info.filename.split('/')\n",
    "                if  re.sub(r'^.*?/', '/', file_info.filename).startswith(folder_to_extract): \n",
    "                    # Extract only the specified folder\n",
    "                    file_data = zipf.read(file_info.filename)  \n",
    "                    if folder_prefix != \"\":\n",
    "                        parts.remove(remove_folder_prefix)\n",
    "                    output_zipf.writestr(('/'.join(parts[1:])), file_data)\n",
    "                    file_count += 1\n",
    "\n",
    "def uncompress_zip_to_folder(zip_path, extract_to):\n",
    "    # Ensure the directory for extraction exists\n",
    "    os.makedirs(extract_to, exist_ok=True)\n",
    "    \n",
    "    # Uncompress all files from the zip into the specified folder\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_to)\n",
    "    \n",
    "    # Delete the original zip file\n",
    "    os.remove(zip_path)\n",
    "\n",
    "download_folder_as_zip(repo_owner, repo_name, output_zip = \"./builtin/src/src.zip\", branch = branch, folder_to_extract= f\"{folder_prefix}/src\", remove_folder_prefix = f\"{folder_prefix}\")\n",
    "uncompress_zip_to_folder(zip_path = \"./builtin/src/src.zip\", extract_to= \"./builtin\")\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a854ac9d-357a-494a-a47a-45bbee069625",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# CLI Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8c0e34",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# FABRIC CLI Utilities\n",
    "# -------------------------------\n",
    "\n",
    "def run_fab_command(command, capture_output=False, silently_continue=False, raw_output=False):\n",
    "    \"\"\"\n",
    "    Executes a Fabric CLI command with optional output capture and error handling.\n",
    "    \"\"\"\n",
    "    result = subprocess.run([\"fab\", \"-c\", command], capture_output=capture_output, text=True)\n",
    "    if not silently_continue and (result.returncode > 0 or result.stderr):\n",
    "        raise Exception(f\"Error running fab command. exit_code: '{result.returncode}'; stderr: '{result}'\")\n",
    "    if capture_output:\n",
    "        return result if raw_output else result.stdout.strip()\n",
    "    return None\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Workspace Management\n",
    "# -------------------------------\n",
    "\n",
    "def get_workspace_id_by_name(workspace_name):\n",
    "    \"\"\"\n",
    "    Retrieves the workspace ID by its display name.\n",
    "    \"\"\"\n",
    "    result = run_fab_command(\"api -X get workspaces/\", capture_output=True, silently_continue=True)\n",
    "    workspaces = json.loads(result)[\"text\"][\"value\"]\n",
    "    normalized_name = workspace_name.strip().lower()\n",
    "    match = next((w for w in workspaces if w['displayName'].strip().lower() == normalized_name), None)\n",
    "    return match['id'] if match else None\n",
    "\n",
    "\n",
    "def ensure_workspace_exists(workspace):\n",
    "    \"\"\"\n",
    "    Ensures the workspace exists; creates it if not found.\n",
    "    \"\"\"\n",
    "    workspace_name = workspace['name']\n",
    "    workspace_id = get_workspace_id_by_name(workspace_name)\n",
    "    if workspace_id:\n",
    "        print(f\" - Workspace '{workspace_name}' found. Workspace ID: {workspace_id}- assign capacity: {workspace['capacity_name']}\")\n",
    "        run_fab_command(f'assign \".capacities/{workspace[\"capacity_name\"]}.Capacity\" -W \"{workspace_name}.Workspace\" -f', silently_continue=True)\n",
    "        return workspace_id, \"exists\"\n",
    "\n",
    "    print(f\" - Workspace '{workspace_name}' not found. Creating new workspace...\")\n",
    "    run_fab_command(f'mkdir \"{workspace_name}.workspace\" -P capacityName=\"{workspace[\"capacity_name\"]}\"', silently_continue=True)\n",
    "    workspace_id = get_workspace_id_by_name(workspace_name)\n",
    "    if workspace_id:\n",
    "        print(f\" - Created workspace '{workspace_name}'. ID: {workspace_id}\")\n",
    "        return workspace_id, \"created\"\n",
    "    else:\n",
    "        raise RuntimeError(f\"Workspace '{workspace_name}' could not be created or found.\")\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Item Utilities\n",
    "# -------------------------------\n",
    "\n",
    "def fab_get_id(workspace_name, name):\n",
    "    \"\"\"\n",
    "    Retrieves the item ID from a workspace.\n",
    "    \"\"\"\n",
    "    return run_fab_command(f\"get /{workspace_name}.Workspace/{name} -q id\", capture_output=True, silently_continue=True)\n",
    "\n",
    "\n",
    "def fab_get_display_name(workspace_name, name):\n",
    "    \"\"\"\n",
    "    Retrieves the display name of an item.\n",
    "    \"\"\"\n",
    "    return run_fab_command(f\"get /{workspace_name}.Workspace/{name} -q displayName\", capture_output=True, silently_continue=True)\n",
    "\n",
    "\n",
    "def fab_get_items(workspace_id, item_id=''):\n",
    "    \"\"\"\n",
    "    Retrieves item definitions or lists from a workspace.\n",
    "    \"\"\"\n",
    "    if item_id:\n",
    "        return run_fab_command(f\"api -X post workspaces/{workspace_id}/items/{item_id}/getDefinition\", capture_output=True, silently_continue=True)\n",
    "    return run_fab_command(f\"api -X get workspaces/{workspace_id}/items/{item_id}\", capture_output=True, silently_continue=True)\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Description and Identity Assignment\n",
    "# -------------------------------\n",
    "\n",
    "def assign_workspace_description(workspace):\n",
    "    \"\"\"\n",
    "    Assigns a standard description to the workspace.\n",
    "    \"\"\"\n",
    "    workspace_name = workspace['name']\n",
    "    payload = 'Important: The items in this workspace are automatically generated by the FMD Framework. Each time the setup notebook is executed, all changes will be overwritten. For more information, please visit https://github.com/edkreuk/FMD_FRAMEWORK.'\n",
    "    run_fab_command(f'set \"/{workspace_name}.workspace -q description -i {payload} -f', silently_continue=True)\n",
    "    print(f\" - Description applied to '{workspace_name}'\")\n",
    "\n",
    "\n",
    "def assign_item_description(workspace, item):\n",
    "    \"\"\"\n",
    "    Assigns a standard description to an item.\n",
    "    \"\"\"\n",
    "    workspace_name = workspace['name']\n",
    "    payload = 'Note: This item was initially generated by the FMD Framework. Any modifications may introduce breaking changes. For further details, please refer to the documentation at https://github.com/edkreuk/FMD_FRAMEWORK.'\n",
    "    run_fab_command(f'set \"/{workspace_name}.workspace/{item} -q description -i {payload} -f', silently_continue=True)\n",
    "    print(f\" - Description applied to {item} in '{workspace_name}'\")\n",
    "\n",
    "\n",
    "def assign_roles(workspace):\n",
    "    \"\"\"\n",
    "    Assigns roles to principals in the workspace.\n",
    "    \"\"\"\n",
    "    workspace_path = f\"/{workspace['name']}.workspace\"\n",
    "    print(f\" - Assigning Workspace roles\")\n",
    "    for role in workspace['roles']:\n",
    "        try:\n",
    "            print(f\"Assigning role '{role['role']}' to '{role['principal']['displayName']}' in workspace '{workspace['name']}'\")\n",
    "            run_fab_command(\n",
    "                f'acl set \"{workspace_path}\" -I {role[\"principal\"][\"id\"]} -R {role[\"role\"]} -f',\n",
    "                silently_continue=True\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\" - Failed to assign role: {e}\")\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Deployment functions\n",
    "# -------------------------------\n",
    "\n",
    "def deploy_workspaces(workspace, environment_name, old_id, mapping_table, tasks):\n",
    "    \"\"\"\n",
    "    Deploys a workspace by ensuring its existence, assigning identity, roles, and description.\n",
    "    Updates the mapping table and logs the deployment task.\n",
    "\n",
    "    Parameters:\n",
    "    - workspace (dict): Workspace configuration including name and capacity.\n",
    "    - environment_name (str): Target environment name.\n",
    "    - old_id (str): Previous workspace ID to be replaced.\n",
    "    - mapping_table (list): List to store ID mappings.\n",
    "    - tasks (list): List to store task execution logs.\n",
    "    \"\"\"\n",
    "    start = time()\n",
    "    print(\"\\n#############################################\")\n",
    "    print(f\" - Processing: workspace {workspace['name']}\")\n",
    "\n",
    "    workspace_id, status = ensure_workspace_exists(workspace)\n",
    "    workspace[\"id\"] = workspace_id\n",
    "\n",
    "    print(\"--------------------------\")\n",
    "    print(f\"Updating Mapping Table: {environment_name}\")\n",
    "    mapping_table.append({\n",
    "        \"Description\": workspace['name'],\n",
    "        \"environment\": environment_name,\n",
    "        \"old_id\": old_id,\n",
    "        \"new_id\": workspace_id\n",
    "    })\n",
    "    mapping_table.append({\n",
    "        \"Description\": workspace['name'],\n",
    "        \"environment\": environment_name,\n",
    "        \"old_id\": \"00000000-0000-0000-0000-000000000000\",\n",
    "        \"new_id\": workspace_id\n",
    "    })\n",
    "\n",
    "    assign_roles(workspace)\n",
    "    assign_workspace_description(workspace)\n",
    "\n",
    "    tasks.append({\n",
    "        \"task_name\": f\"Update workspace {workspace['name']}\",\n",
    "        \"task_duration\": int(time() - start),\n",
    "        \"status\": \"success\"\n",
    "    })\n",
    "\n",
    "\n",
    "def deploy_folder_to_workspace(workspace: dict, folder_path: str, environment_name: str,\n",
    "                               mapping_table: list, lakehouse_schema_enabled: bool) -> None:\n",
    "    \"\"\"\n",
    "    Deploy all items from a given folder into the target workspace.\n",
    "\n",
    "    Args:\n",
    "        workspace (dict): Workspace configuration.\n",
    "        folder_path (str): Path to the folder containing items to deploy.\n",
    "        environment_name (str): Target environment.\n",
    "        mapping_table (list): Deployment mapping table.\n",
    "        lakehouse_schema_enabled (bool): Whether to enable lakehouse schema creation.\n",
    "    \"\"\"\n",
    "    if not os.path.isdir(folder_path):\n",
    "        print(f\"Folder not found: {folder_path}\")\n",
    "        return\n",
    "\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        deploy_file_to_workspace(\n",
    "            workspace=workspace,\n",
    "            file_path=file_path,\n",
    "            environment_name=environment_name,\n",
    "            mapping_table=mapping_table,\n",
    "            lakehouse_schema_enabled=lakehouse_schema_enabled,\n",
    "        )\n",
    "\n",
    "def deploy_file_to_workspace(workspace: dict, file_path: str, environment_name: str,\n",
    "                             mapping_table: list, lakehouse_schema_enabled: bool) -> None:\n",
    "    \"\"\"\n",
    "    Deploy a single file into the given workspace.\n",
    "    \"\"\"\n",
    "    workspace_name = workspace[\"name\"]\n",
    "    file_name = os.path.basename(file_path)\n",
    "    item_name, item_extension = os.path.splitext(file_name)\n",
    "\n",
    "    print(\"\\n#############################################\")\n",
    "    print(f\"Deploying in {workspace_name}: {item_name}\")\n",
    "\n",
    "    cli_parameter = \"\"\n",
    "\n",
    "    if \"Notebook\" in item_extension:\n",
    "        cli_parameter += \" --format .py\"\n",
    "        result = run_fab_command(\n",
    "            f'import / {workspace_name}.Workspace/{item_name}.Notebook -i \"{file_path}\" -f {cli_parameter}',\n",
    "            capture_output=True, silently_continue=True, raw_output=False\n",
    "        )\n",
    "        assign_item_description(workspace, item_name)\n",
    "        new_id = fab_get_id(workspace_name, item_name)\n",
    "\n",
    "    elif \"Lakehouse\" in item_extension:\n",
    "        param = \"-P enableschemas=true\" if lakehouse_schema_enabled else \"-P\"\n",
    "        result = run_fab_command(\n",
    "            f'create {workspace_name}.Workspace/{item_name} {param}',\n",
    "            capture_output=True, silently_continue=True, raw_output=False\n",
    "        )\n",
    "        assign_item_description(workspace, item_name)\n",
    "        new_id = fab_get_id(workspace_name, item_name)\n",
    "\n",
    "    elif \"DataPipeline\" in item_extension:\n",
    "        result = run_fab_command(\n",
    "            f'import {workspace_name}.Workspace/{item_name}.DataPipeline -i \"{file_path}\" -f {cli_parameter}',\n",
    "            capture_output=True, silently_continue=True, raw_output=False\n",
    "        )\n",
    "        assign_item_description(workspace, item_name)\n",
    "        new_id = fab_get_id(workspace_name, item_name)\n",
    "\n",
    "    elif \"VariableLibrary\" in item_extension:\n",
    "        print(f\"Creating {workspace_name}: {item_name}\")\n",
    "        result = run_fab_command(\n",
    "            f'import / {workspace_name}.Workspace/{item_name}.VariableLibrary -i \"{file_path}\" -f',\n",
    "            capture_output=True, silently_continue=True, raw_output=False\n",
    "        )\n",
    "        new_id = fab_get_id(workspace_name, item_name)\n",
    "\n",
    "    else:\n",
    "        print(f\" Skipping {item_name} (unknown type)\")\n",
    "        return\n",
    "\n",
    "    print(result)\n",
    "\n",
    "    mapping_table.append({\n",
    "        \"Description\": item_name,\n",
    "        \"environment\": environment_name,\n",
    "        \"old_id\": \"N/A\",\n",
    "        \"new_id\": new_id,\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1bb51f2",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "def create_deployment_pipeline(workspace, environment_name, deployment_pipeline_template):\n",
    "    if workspace[\"deployment_pipeline\"]:\n",
    "        workspace_id, status = ensure_workspace_exists(workspace)\n",
    "        workspace_name_regex = workspace[\"name\"] # regex to 'ensure' same workspaces get assigned to the same deployment pipeline\n",
    "        result_regex = re.sub(r'\\s*\\(.*$', \"\", workspace_name_regex)\n",
    "        \n",
    "        print(\"\\n#############################################\")\n",
    "        print(f\" - Creating deployment pipeline: {result_regex}\")\n",
    "        \n",
    "        pipeline_id = get_existing_pipeline(result_regex)\n",
    "        \n",
    "        if pipeline_id:\n",
    "            print(f\" - Pipeline already exists: {result_regex} (ID: {pipeline_id})\")\n",
    "        else:\n",
    "            print(f\" - Creating new pipeline: {result_regex}\")\n",
    "            template = deployment_pipeline_template.copy()\n",
    "            template[\"displayName\"] = template_deployment_pipeline[\"displayName\"].replace(\"{workspace_name}\", result_regex)\n",
    "\n",
    "            payload = json.dumps(template)\n",
    "            response = run_fab_command(f\"api -X post deploymentPipelines  -i {payload}\", capture_output=True, silently_continue=True)\n",
    "            response_dict = json.loads(response)\n",
    "            pipeline_id = response_dict[\"text\"][\"id\"]\n",
    "        \n",
    "        assign_user_to_deployment_pipeline(workspace, pipeline_id)\n",
    "        assign_workspace_to_pipeline(pipeline_id, workspace_id, stage_name=environment_name)\n",
    "        deploy_pipeline_artifacts(pipeline_id)\n",
    "        print(\"---------------------------------------------\\n\")\n",
    "\n",
    "\n",
    "def get_existing_pipeline(workspace_name: str) -> str | None:\n",
    "    \"\"\"Get the ID of an existing pipeline matching the workspace name.\"\"\"\n",
    "    try:\n",
    "        response = run_fab_command(\"api deploymentPipelines\", capture_output=True, silently_continue=True)\n",
    "        pipelines = json.loads(response).get(\"text\", {}).get(\"value\", [])\n",
    "        \n",
    "        for pipeline in pipelines:\n",
    "            if workspace_name in pipeline.get(\"displayName\", \"\"):\n",
    "                return pipeline[\"id\"]\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "def assign_user_to_deployment_pipeline(workspace, pipeline_id):\n",
    "    for role in workspace['roles']:\n",
    "        try:\n",
    "            # role['principal'].pop('displayName', None)\n",
    "            print(f\" - Assigning role '{role['role']}' to '{role['principal']['displayName']}' to pipeline '{pipeline_id}'\")\n",
    "            role = json.dumps(role)\n",
    "            response = run_fab_command(f\"api -X post deploymentPipelines/{pipeline_id}/roleAssignments -i {role}\", capture_output=True, silently_continue=True)\n",
    "            result = json.loads(response)\n",
    "            if result.get(\"status_code\") == 200:\n",
    "                continue\n",
    "            else:\n",
    "                print(f'    ! {result.get(\"text\", {}).get(\"message\", \"\")} \\n')\n",
    "        except Exception as e:\n",
    "            print(f\"    - Failed to assign role: {e}\")\n",
    "\n",
    "\n",
    "def assign_workspace_to_pipeline(pipeline_id, workspace_id, stage_name):\n",
    "    \"\"\"Assign a workspace to a pipeline. Returns None\"\"\"\n",
    "    pipeline_stages = get_pipeline_stages(pipeline_id)\n",
    "    # Find stage id by displayName\n",
    "    stage_id = next(\n",
    "        (s[\"id\"] for s in pipeline_stages if s.get(\"displayName\", \"\").lower() == stage_name.lower()),\n",
    "        None\n",
    "    )\n",
    "    if not stage_id:\n",
    "        raise ValueError(f\"Stage '{stage_name}' not found in pipeline {pipeline_id}\")\n",
    "    payload = {\"workspaceId\": workspace_id}\n",
    "    payload_str = json.dumps(payload)\n",
    "    response = run_fab_command(f\"api -X post deploymentPipelines/{pipeline_id}/stages/{stage_id}/assignWorkspace -i {payload_str}\", capture_output=True, silently_continue=True)\n",
    "    result = json.loads(response)\n",
    "    if result.get(\"status_code\") == 200:\n",
    "        print(f\" - Assigned workspace (ID: {workspace_id}) to the pipeline stage (ID: {stage_id})\")\n",
    "    else:\n",
    "        print(f' ! Assigning workspace error: {result.get(\"text\", {}).get(\"message\", \"\")}')\n",
    "\n",
    "\n",
    "def get_pipeline_stages(pipeline_id):\n",
    "    \"\"\"Check all pipelines stages for a specific pipeline and returns all stages.\"\"\"\n",
    "    response = run_fab_command(f\"api deploymentPipelines/{pipeline_id}/stages\", capture_output=True, silently_continue=True)\n",
    "    pipeline_stages = json.loads(response).get(\"text\", {}).get(\"value\", [])\n",
    "    return pipeline_stages\n",
    "\n",
    "\n",
    "def pipeline_is_running(pipeline_id):\n",
    "    \"\"\"Check if there is an ongoing deployment for this pipeline and returns the most recent status.\"\"\"\n",
    "    response = run_fab_command(f\"api deploymentPipelines/{pipeline_id}/operations\", capture_output=True, silently_continue=True)\n",
    "    operations = json.loads(response).get(\"text\", {}).get(\"value\", [])\n",
    "    latest_op = max(operations, key=lambda op: op.get(\"lastUpdatedTime\", \"\"), default=None)\n",
    "    status = latest_op.get(\"status\") if latest_op else None \n",
    "    return status \n",
    "\n",
    "\n",
    "def deploy_pipeline_artifacts(pipeline_id):\n",
    "    pipeline_stages = get_pipeline_stages(pipeline_id)\n",
    "    # Check if any stage is missing a workspaceId\n",
    "    if any(\"workspaceId\" not in stage for stage in pipeline_stages):\n",
    "        print(\" - Not all stages have a workspace assigned to them, continuing deployment...\")\n",
    "        return\n",
    "    \n",
    "    status = pipeline_is_running(pipeline_id)\n",
    "    if status in (\"Running\", \"NotStarted\"):\n",
    "        print(f\" - Pipeline {pipeline_id} is currently {status}. Skipping deployment.\")\n",
    "        return\n",
    "    \n",
    "    stages_to_deploy = [{\"id\": s[\"id\"], \"name\": s[\"displayName\"], \"order\": s[\"order\"]} for s in pipeline_stages]\n",
    "    for i in range(len(stages_to_deploy) - 1):\n",
    "        source = stages_to_deploy[i]\n",
    "        target = stages_to_deploy[i+1]\n",
    "\n",
    "        payload = {\n",
    "            \"sourceStageId\": source[\"id\"],\n",
    "            \"targetStageId\": target[\"id\"],\n",
    "            \"note\": \"Deployed via DoD platform\"\n",
    "        }\n",
    "        payload_str = json.dumps(payload)\n",
    "\n",
    "        print(f\" - Deploying all artifacts from {source['name']} to {target['name']}\")\n",
    "        run_fab_command(f\"api -X post deploymentPipelines/{pipeline_id}/deploy -i {payload_str}\", capture_output=True, silently_continue=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c094ee",
   "metadata": {},
   "source": [
    "# Deployment of workspaces, items and deployment pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5678d4",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "for environment in environments:\n",
    "    print(f\"--------------------------\")\n",
    "    print(f\"Updating Workspace: {environment['environment_name']}\")\n",
    "    deploy_workspaces(workspace=environment['workspaces']['code'], environment_name=environment['environment_name'], old_id=empty_guid, mapping_table=mapping_table, tasks=tasks)\n",
    "    deploy_workspaces(workspace=environment['workspaces']['data'], environment_name=environment['environment_name'], old_id=empty_guid, mapping_table=mapping_table, tasks=tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dff91d0",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "for environment in environments:\n",
    "    if environment[\"environment_name\"].lower() != ENVIRONMENT_NAME:\n",
    "        continue  # skip deployment for other environments because we will use the deployment pipeline to deploy the items.\n",
    "\n",
    "    print(\"\\n--------------------------\")\n",
    "    print(f\"Processing environment: {environment['environment_name']}\")\n",
    "\n",
    "    workspace = environment[\"workspaces\"][\"code\"]\n",
    "    deploy_folder_to_workspace(\n",
    "        workspace=workspace,\n",
    "        folder_path=resource_dir,\n",
    "        environment_name=environment[\"environment_name\"],\n",
    "        mapping_table=mapping_table,\n",
    "        lakehouse_schema_enabled=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8ec330",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "for environment in environments:\n",
    "    create_deployment_pipeline(workspace=environment['workspaces']['code'], environment_name=environment['environment_name'], deployment_pipeline_template=template_deployment_pipeline)\n",
    "    create_deployment_pipeline(workspace=environment['workspaces']['data'], environment_name=environment['environment_name'], deployment_pipeline_template=template_deployment_pipeline)"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "jupyter_kernel_name": "python3.11",
   "name": "jupyter"
  },
  "kernelspec": {
   "display_name": "Jupyter",
   "name": "jupyter"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "jupyter_python",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
